import pandas as pd  # å¯¼å…¥ pandas åº“ç”¨äºæ•°æ®å¤„ç†
import numpy as np  # å¯¼å…¥ numpy åº“ç”¨äºæ•°å€¼è®¡ç®—
import torch  # å¯¼å…¥ PyTorch åº“ç”¨äºæ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹
import torch.nn.functional as F  # å¯¼å…¥ PyTorch ä¸­çš„åŠŸèƒ½æ¨¡å—ï¼Œä¸»è¦åŒ…å«ç¥ç»ç½‘ç»œå±‚
import torch.utils.data as Data  # å¯¼å…¥ PyTorch çš„æ•°æ®å¤„ç†æ¨¡å—
from sklearn.model_selection import train_test_split  # ä» scikit-learn ä¸­å¯¼å…¥ç”¨äºæ•°æ®åˆ†å‰²çš„å‡½æ•°
from sklearn.preprocessing import MinMaxScaler  # ä» scikit-learn ä¸­å¯¼å…¥æ•°æ®æ ‡å‡†åŒ–æ¨¡å—
import datetime  # å¯¼å…¥ datetime åº“å¤„ç†æ—¥æœŸå’Œæ—¶é—´
import pickle  # å¯¼å…¥ pickle åº“ç”¨äºæ•°æ®åºåˆ—åŒ–
import time  # å¯¼å…¥ time åº“ç”¨äºå¤„ç†æ—¶é—´ç›¸å…³çš„ä»»åŠ¡
import os  # å¯¼å…¥ os åº“ç”¨äºæ“ä½œç³»ç»Ÿçº§åˆ«çš„æ¥å£ï¼Œå¦‚è¯»å†™æ–‡ä»¶
import torch.nn as nn
import torch
from datetime import datetime
import pickle
from collections import defaultdict
from math import radians, sin, cos, asin, sqrt
from tqdm import tqdm
import pandas as pd
from collections import OrderedDict

# å®šä¹‰å¤„ç†å˜é•¿æ•°æ®çš„å‡½æ•°


def sliding_varlen(data, batch_size):
    print("function: sliding_varlen() is running...")

    def timedelta(time1, time2):  # å®šä¹‰è®¡ç®—æ—¶é—´å·®çš„å‡½æ•°
        t1 = datetime.datetime.strptime(str(time1), '%a %b %d %H:%M:%S %z %Y')
        t2 = datetime.datetime.strptime(str(time2), '%a %b %d %H:%M:%S %z %Y')
        delta = t1-t2
        time_delta = datetime.timedelta(
            days=delta.days, seconds=delta.seconds).total_seconds()
        return time_delta/3600  # å°†æ—¶é—´å·®è½¬æ¢ä¸ºå°æ—¶å¹¶è¿”å›

    def get_entropy(x):  # å®šä¹‰è®¡ç®—ç†µçš„å‡½æ•°
        x_value_list = set([x[i] for i in range(x.shape[0])])
        ent = 0.0
        for x_value in x_value_list:
            p = float(x[x == x_value].shape[0]) / x.shape[0]
            logp = np.log2(p)
            ent -= p * logp
        return ent

#################################################################################
    # 1ã€sort the raw data in chronological order
    # NOTE1.
    timestamp = []
    hour = []
    day = []
    week = []
    week_n_list = []

    #TODO æŒ‰useridå’Œtimesstampå†æ’åº

    for i in range(len(data)):
        times = data['timestamp'].values[i]
        # Correct format string
        t = datetime.strptime(times, '%Y-%m-%d %H:%M:%S%z')
        year = int(t.strftime('%Y'))
        day_i = int(t.strftime('%j'))
        week_i = t.weekday()
        hour_i = int(t.strftime('%H'))
        week_n = t.isocalendar()[1]  # 1-52

        # Adjust hour for weekends
        if week_i == 0 or week_i == 6:
            hour_i = hour_i + 24

        # Adjust day for leap years
        if year == 2013:
            day_i = day_i + 366

        # Append extracted values to corresponding lists
        timestamp.append(time.mktime(t.timetuple()))
        day.append(day_i)
        hour.append(hour_i)
        week.append(week_i)
        week_n_list.append(week_n)

    data['timestamp'] = timestamp  # æ·»åŠ æ—¶é—´æˆ³
    data['hour'] = hour  # æ·»åŠ å°æ—¶ä¿¡æ¯
    print(data['hour'][10:])
    data['day'] = day  # æ·»åŠ å¤©ä¿¡æ¯
    data['week'] = week  # æ·»åŠ å‘¨ä¿¡æ¯
    data['week_n_list'] = week_n_list
#! åµŒå…¥å‘¨ï¼Œå’Œå‘¨é¢„æµ‹ç­‰ç­‰ä¸å‘¨ç›¸å…³çš„ TODO
    # TODO çœ‹ä¸€ä¸‹PG2Netçš„æ—¶é—´åˆ’åˆ†ç‰‡æ˜¯æ€ä¹ˆæ¥çš„

    # data.sort_values(by = 'timestamp',inplace=True,ascending = True)	# å°†æ•°æ®æŒ‰æ—¶é—´æˆ³å‡åºæ’åº
    # TODO: è¿™é‡Œæ—¢ç„¶ä¸æ’åºäº†ï¼Œé‚£å°±è¾“å‡ºä¸€ä¸‹çœ‹çœ‹ï¼Œæ˜¯å¦ç¬¦åˆé¢„æœŸ

#################################################################################
    # 2ã€filter users and POIs
    # NOTE2.ç­›é€‰ç”¨æˆ·å’Œåœ°ç‚¹ï¼ˆPOIï¼‰
    data['userid'] = data['userid'].rank(method='dense').values  # å°†ç”¨æˆ·IDè½¬æ¢ä¸ºå¯†é›†æ’å
    data['userid'] = data['userid'].astype(int)
    data['venueid'] = data['venueid'].rank(
        method='dense').values  # !å°†åœ°ç‚¹IDè½¬æ¢ä¸ºå¯†é›†æ’å ä»1å¼€å§‹ï¼Œvenueidç›¸åŒåˆ™æ’åç›¸åŒ
    data['venueid'] = data['venueid'].astype(int)
    for venueid, group in data.groupby('venueid'):
        indexs = group.index
        if len(set(group['catid'].values)) > 1:  # å¦‚æœåŒä¸€åœ°ç‚¹æœ‰å¤šä¸ªç±»åˆ«ID
            for i in range(len(group)):
                data.loc[indexs[i], 'catid'] = group.loc[indexs[0]
                                                         ]['catid']  # ç»Ÿä¸€ç±»åˆ«ID

    data = data.drop_duplicates()  # åˆ é™¤é‡å¤é¡¹
    data['catid'] = data['catid'].rank(method='dense').values  # å°†ç±»åˆ«IDè½¬æ¢ä¸ºå¯†é›†æ’å
    data['catid'] = data['catid'].astype(int)
# data['catid'] = data['catid'].rank(method='dense').values.astype(int)  # å°†ç±»åˆ«IDè½¬æ¢ä¸ºå¯†é›†æ’å

    # data['sccode'] = data['sccode'].rank(method='dense').values
    # data['sccode'] = data['sccode'].astype(int)

    # data['week_n_list'] æ— é¡»rank, 1-52

#################################################################################
    poi_cat = data[['venueid', 'catid']]  # TODO è¦ä¸è¦3ä¸ªä¸€èµ·å»é‡
    poi_cat = poi_cat.drop_duplicates()  # å»é‡
    poi_cat = poi_cat.sort_values(by='venueid')
    # poi_cat.to_csv("t_split.txt", sep='\t', index=False)
    cat_candidate = torch.Tensor(poi_cat['catid'].values)  # åˆ›å»ºç±»åˆ«å€™é€‰çš„å¼ é‡

    with open('../data/cat_candidate.pk', 'wb') as f:  # å°†ç±»åˆ«å€™é€‰æ•°æ®åºåˆ—åŒ–åˆ°æ–‡ä»¶
        pickle.dump(cat_candidate, f)

    # å»ºç«‹venueIDå’Œç»çº¬åº¦çš„æ˜ å°„æ•°æ®ä¿å­˜åˆ°venueid2coor_1type.pklä¸­
    venueid2coor = OrderedDict()
    catid2seqnum = OrderedDict()
    total_rows = len(data)
    with tqdm(total=total_rows, desc="Processing rows") as pbar:
        seq1 = 0
        seq2 = 0
        for i in range(total_rows):
            venueid = data['venueid'].values[i]
            if venueid not in venueid2coor:
                venueid2coor[venueid] = seq1
                seq1 += 1
            catid = data['catid'].values[i]
            if catid not in catid2seqnum:
                catid2seqnum[catid] = seq2
                seq2 += 1
            pbar.update(1)
    with open('../data/venueid2coor2seqnumber_1type.pkl', 'wb') as f:
        pickle.dump(venueid2coor, f)
    print("venueid2coor2seqnumber_1type.pkl Done")
    with open('../data/catid2seqnum.pkl', 'wb') as f:
        pickle.dump(catid2seqnum, f)
    print("catid2seqnum.pkl Done")

    # 3ã€split data into train set and test set.
    #    extract features of each session for classification
# NOTE3.æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæå–æ¯ä¸ªä¼šè¯çš„ç‰¹å¾ç”¨äºåˆ†ç±»
    vocab_size_poi = int(max(data['venueid'].values))  # è·å–åœ°ç‚¹çš„è¯æ±‡å¤§å°
    vocab_size_cat = int(max(data['catid'].values))  # è·å–ç±»åˆ«çš„è¯æ±‡å¤§å°
    vocab_size_user = int(max(data['userid'].values))  # è·å–ç”¨æˆ·çš„è¯æ±‡å¤§å°
    vocab_size_week_n = int(max(data['week_n_list'].values))  # è·å–å‘¨çš„è¯æ±‡å¤§å°
    vocab_size_hour = int(max(data['hour'].values))

    print('vocab_size_poi: ', vocab_size_poi)
    print('vocab_size_cat: ', vocab_size_cat)
    print('vocab_size_user: ', vocab_size_user)
    print('vocab_size_week_n: ', vocab_size_week_n)
    print('vocab_size_hour:', vocab_size_hour)

# train è®­ç»ƒç”¨çš„
    train_x = []
    train_x_cat = []
    train_x_week_n = []
    train_y = []
    train_hour = []
    train_userid = []
    train_indexs = []

# the hour and week to be predicted
    train_hour_pre = []  # é¢„æµ‹çš„å°æ—¶
    train_week_pre = []  # é¢„æµ‹çš„å‘¨

# test æµ‹è¯•ç”¨çš„
    test_x = []
    test_x_cat = []
    test_x_week_n = []
    test_y = []
    test_hour = []
    test_userid = []
    test_indexs = []

# the hour and week to be predicted
    test_hour_pre = []  # é¢„æµ‹çš„å°æ—¶
    test_week_pre = []  # é¢„æµ‹çš„å‘¨

    long_term = {}  # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ï¼Œç”¨æ¥å­˜å‚¨é•¿æœŸç‰¹å¾

    long_term_feature = []  # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨é•¿æœŸç‰¹å¾æ•°æ®

    data_train = {}  # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨è®­ç»ƒæ•°æ®
    train_idx = {}  # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨æ‰€æœ‰ç”¨æˆ·çš„è®­ç»ƒæ•°æ®ç´¢å¼•

    data_test = {}  # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨æµ‹è¯•æ•°æ®
    test_idx = {}  # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨æ‰€æœ‰ç”¨æˆ·çš„æµ‹è¯•æ•°æ®ç´¢å¼•

    # å­˜å‚¨æ•°æ®çš„åŸºæœ¬ä¿¡æ¯ï¼ŒåŒ…æ‹¬POIã€ç±»åˆ«å’Œç”¨æˆ·çš„å¤§å°
    data_train['datainfo'] = {'size_poi': vocab_size_poi+1, 'size_cat': vocab_size_cat +
                              1, 'size_user': vocab_size_user+1, 'size_week_n': vocab_size_week_n+1}  # TODO +1?

    len_session = 10  # NOTE è®¾ç½®ä¼šè¯çª—å£çš„é•¿åº¦ä¸º10
    user_lastid = {}  # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ï¼Œç”¨æ¥å­˜å‚¨ç”¨æˆ·çš„æœ€åä¸€ä¸ªID
###################################################################################
    # split data åˆ†å‰²æ•°æ®
    # TODO ä¹‹å‰å·²ç»æ’å¥½åºäº†ï¼Œè¿™é‡Œè¿˜è¦å†æ’å—ï¼Œgroupè¦æœ‰ï¼Œä½†è¿™æ ·ä¼šä¸ä¼šæ‰“ä¹±æ—¶é—´é¡ºåº
    for uid, group in data.groupby('userid'):  # å¯¹æ•°æ®æŒ‰ç”¨æˆ·IDè¿›è¡Œåˆ†ç»„
        data_train[uid] = {}  # åˆå§‹åŒ–è¯¥ç”¨æˆ·çš„è®­ç»ƒæ•°æ®é›†
        data_test[uid] = {}  # åˆå§‹åŒ–è¯¥ç”¨æˆ·çš„æµ‹è¯•æ•°æ®é›†
        user_lastid[uid] = []  # åˆå§‹åŒ–è¯¥ç”¨æˆ·çš„æœ€åä¸€ä¸ªIDåˆ—è¡¨
        inds_u = group.index.values  # è·å–è¯¥ç”¨æˆ·çš„æ‰€æœ‰ç´¢å¼•å€¼
    # NOTE æŒ‰ç…§ç”¨æˆ·çš„checkinsæ¥è¿›è¡Œåˆ†å‰²çš„
        # NOTE ç”¨80%çš„æ•°æ®ä½œä¸ºè®­ç»ƒé›† å•ç”¨æˆ·  åæœŸä¸ºæé«˜ç²¾åº¦å¯é è¿‘ä¸åŸå¸‚é—´éš”æµ‹è¯•ä¸€ä¸‹
        split_ind = int(np.floor(0.8 * len(inds_u)))
        train_inds = inds_u[:split_ind]  # è®­ç»ƒé›†çš„ç´¢å¼•
        test_inds = inds_u[split_ind:]  # æµ‹è¯•é›†çš„ç´¢å¼•

    # get the features of POIs for user uid
        # long_term_feature.append(get_features(group.loc[train_inds]))
        # è·å–ç”¨æˆ·uidçš„POIç‰¹å¾
        long_term[uid] = {}
        lt_data = group.loc[train_inds]  # è·å–è®­ç»ƒé›†æ•°æ®
        # NOTE:é•¿æœŸç‰¹å¾ï¼šloc\hour\week\category
        long_term[uid]['loc'] = torch.LongTensor(
            lt_data['venueid'].values).cuda()  # å­˜å‚¨ä½ç½®ä¿¡æ¯
        long_term[uid]['hour'] = torch.LongTensor(
            lt_data['hour'].values).cuda()  # å­˜å‚¨å°æ—¶ä¿¡æ¯
        long_term[uid]['week'] = torch.LongTensor(
            lt_data['week'].values).cuda()  # å­˜å‚¨æ˜ŸæœŸä¿¡æ¯
        long_term[uid]['category'] = torch.LongTensor(
            lt_data['catid'].values).cuda()  # å­˜å‚¨ç±»åˆ«ä¿¡æ¯

    # split the long sessions to some short ones with len_session =
    # å°†é•¿ä¼šè¯åˆ†å‰²æˆå¤šä¸ªçŸ­ä¼šè¯ï¼Œæ¯ä¸ªä¼šè¯é•¿åº¦ä¸º
        train_inds_split = []
        num_session_train = int(len(train_inds) // len_session)  # è®¡ç®—è®­ç»ƒé›†ä¸­ä¼šè¯çš„æ•°é‡
        for i in range(num_session_train):
            train_inds_split.append(
                train_inds[i * len_session:(i + 1) * len_session])  # åˆ†å‰²ä¼šè¯
        if num_session_train < len(train_inds) / len_session:
            train_inds_split.append(train_inds[-len_session:])  # æ·»åŠ å‰©ä½™çš„è®­ç»ƒæ•°æ®

        train_id = list(range(len(train_inds_split))) 	# ç”Ÿæˆè®­ç»ƒé›†IDåˆ—è¡¨

        test_inds_split = []
        num_session_test = int(len(test_inds) // len_session)  # è®¡ç®—æµ‹è¯•é›†ä¸­ä¼šè¯çš„æ•°é‡
        for i in range(num_session_test):
            test_inds_split.append(
                test_inds[i * len_session:(i + 1) * len_session])  # åˆ†å‰²ä¼šè¯
        if num_session_test < len(test_inds) / len_session:
            test_inds_split.append(test_inds[-len_session:])  # æ·»åŠ å‰©ä½™çš„æµ‹è¯•æ•°æ®

        # ç”Ÿæˆæµ‹è¯•é›†IDåˆ—è¡¨
        test_id = list(range(len(test_inds_split) +
                       len(train_inds_split)))[-len(test_inds_split):]
        # æµ‹è¯•é›†çš„ç´¢å¼•ä»å…¨å±€ç´¢å¼• [3, 4] å¼€å§‹ï¼Œç¡®ä¿äº†è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç´¢å¼•èŒƒå›´ä¸é‡å 

        #
        train_idx[uid] = train_id[1:]  # å­˜å‚¨è®­ç»ƒé›†ç´¢å¼•ï¼ˆæ’é™¤ç¬¬ä¸€ä¸ªï¼Œå› ä¸ºå®ƒæ²¡æœ‰å†å²æ•°æ®ï¼‰-ç”¨æˆ·çš„ç¬¬ä¸€ä¸ª
        test_idx[uid] = test_id  # å­˜å‚¨æµ‹è¯•é›†ç´¢å¼•

        # NOTE:generate data for comparative methods such as deepmove	ä¸º deepmove ç­‰æ¯”è¾ƒæ–¹æ³•ç”Ÿæˆæ•°æ®ï¼Œæ›¿æ¢baselines?è¿™éƒ¨åˆ†åŠ¨ï¼Ÿ
        for ind in train_id:
            if ind == 0:
                continue  # è·³è¿‡ç¬¬ä¸€ä¸ªç´¢å¼•ï¼Œå› ä¸ºæ²¡æœ‰å†å²æ•°æ®
            data_train[uid][ind] = {}  # åˆå§‹åŒ–è¯¥ç´¢å¼•çš„è®­ç»ƒæ•°æ®
            # TODO è¿™ä¸ª[uid][ind]æ˜¯ä»€ä¹ˆï¼Œindæ˜¯ä»€ä¹ˆï¼Ÿ
            history_ind = []
            for i in range(ind):
                history_ind.extend(train_inds_split[i])  # è·å–å½“å‰ç´¢å¼•ä¹‹å‰æ‰€æœ‰ä¼šè¯çš„æ•°æ®
            whole_ind = []
            whole_ind.extend(history_ind)
            whole_ind.extend(train_inds_split[ind])  # åŒ…æ‹¬å½“å‰ä¼šè¯çš„æ•°æ®
            whole_data = group.loc[whole_ind]  # è·å–æ•´ä¸ªæ•°æ®

            loc = whole_data['venueid'].values[:-1]  # ä½ç½®æ•°æ®
            tim = whole_data['hour'].values[:-1]  # æ—¶é—´æ•°æ®
            wek_n = whole_data['week_n_list'].values[:-1]  # å¹´å‘¨æ¬¡æ•°æ®
            target = group.loc[train_inds_split[ind]
                               [1:]]['venueid'].values  # !ç›®æ ‡ä½ç½®æ•°æ® targetåªæ˜¯ç›®æ ‡venueid?

            data_train[uid][ind]['loc'] = torch.LongTensor(
                loc).unsqueeze(1)  # è½¬æ¢ä¸ºå¼ é‡å¹¶å¢åŠ ç»´åº¦
            data_train[uid][ind]['tim'] = torch.LongTensor(
                tim).unsqueeze(1)  # è½¬æ¢ä¸ºå¼ é‡å¹¶å¢åŠ ç»´åº¦
            data_train[uid][ind]['wek_n'] = torch.LongTensor(
                wek_n).unsqueeze(1)
            data_train[uid][ind]['target'] = torch.LongTensor(target)  # è½¬æ¢ä¸ºå¼ é‡
            # ç›®æ ‡æ•°æ®tragetç”¨äºè®¡ç®—æ¨¡å‹æŸå¤±ï¼ŒæŸå¤±å‡½æ•°é€šå¸¸ç›´æ¥æ¥å—ç›®æ ‡å¼ é‡çš„åŸå§‹å½¢çŠ¶

            user_lastid[uid].append(loc[-1])  # æ›´æ–°ç”¨æˆ·çš„æœ€åä¸€ä¸ªä½ç½®ID

        #! here
            group_i = group.loc[train_inds_split[ind]]  # å½“å‰ä¼šè¯çš„æ•°æ®

            # generate data for SHAN
            current_loc = group_i['venueid'].values  # å½“å‰ä½ç½®æ•°æ®
            data_train[uid][ind]['current_loc'] = torch.LongTensor(
                current_loc).unsqueeze(1)  # ? è½¬æ¢ä¸ºå¼ é‡å¹¶å¢åŠ ç»´åº¦ -ä½œä¸ºåºåˆ—è¾“å…¥åˆ°æ¨¡å‹ä¸­

            current_cat = group_i['catid'].values  # å½“å‰ç±»åˆ«æ•°æ®
            # ?data_train[uid][ind]['current_cat'] = torch.LongTensor(current_cat).unsqueeze(1)

            current_wek_n = group_i['week_n_list'].values
            data_train[uid][ind]['current_wek_n'] = torch.LongTensor(
                current_wek_n).unsqueeze(1)  # !Test

            # generate data for my methods. X,Y,time,userid
            train_x.append(current_loc[:-1])  # è®­ç»ƒä½ç½®æ•°æ®
            train_x_cat.append(current_cat[:-1])  # è®­ç»ƒç±»åˆ«æ•°æ®
            train_x_week_n.append(current_wek_n[:-1])
            train_y.append(current_loc[1:])  # è®­ç»ƒç›®æ ‡ä½ç½®æ•°æ®
            train_hour.append(group_i['hour'].values[:-1])  # è®­ç»ƒæ—¶é—´æ•°æ®
            train_userid.append(uid)  # è®­ç»ƒç”¨æˆ·ID
            train_hour_pre.append(group_i['hour'].values[1:])  # é¢„æµ‹çš„å°æ—¶æ•°æ®
            train_week_pre.append(group_i['week'].values[1:])  # é¢„æµ‹çš„æ˜ŸæœŸæ•°æ®
            train_indexs.append(group_i.index.values)  # è®­ç»ƒæ•°æ®ç´¢å¼•

        ##########################
        # data for test
        for ind in test_id:
            data_test[uid][ind] = {}  # åˆå§‹åŒ–è¯¥ç´¢å¼•çš„æµ‹è¯•æ•°æ®
            history_ind = []
            for i in range(len(train_inds_split)):
                history_ind.extend(train_inds_split[i])  # è·å–è®­ç»ƒé›†æ‰€æœ‰ä¼šè¯çš„æ•°æ®
            whole_ind = []
            whole_ind.extend(history_ind)
            # åŒ…æ‹¬æµ‹è¯•é›†å½“å‰ä¼šè¯çš„æ•°æ®
            whole_ind.extend(test_inds_split[ind-len(train_inds_split)])

            whole_data = group.loc[whole_ind]  # è·å–æ•´ä¸ªæ•°æ®

            loc = whole_data['venueid'].values[:-1]  # ä½ç½®æ•°æ®
            tim = whole_data['hour'].values[:-1]  # æ—¶é—´æ•°æ®
            wek_n = whole_data['week_n_list'].values[:-1]  # å¹´å‘¨æ¬¡æ•°æ®
            target = group.loc[test_inds_split[ind -
                                               len(train_inds_split)][1:]]['venueid'].values  # ç›®æ ‡ä½ç½®æ•°æ®

            data_test[uid][ind]['loc'] = torch.LongTensor(
                loc).unsqueeze(1)  # å°†ä½ç½®æ•°æ®è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œç”¨äºå­˜å‚¨åœ¨æµ‹è¯•æ•°æ®å­—å…¸ä¸­
            data_test[uid][ind]['tim'] = torch.LongTensor(
                tim).unsqueeze(1)  # å°†æ—¶é—´æ•°æ®è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œç”¨äºå­˜å‚¨åœ¨æµ‹è¯•æ•°æ®å­—å…¸ä¸­
            data_test[uid][ind]['target'] = torch.LongTensor(
                target)  # å°†ç›®æ ‡æ•°æ®è½¬æ¢ä¸ºå¼ é‡ï¼Œå­˜å‚¨åœ¨æµ‹è¯•æ•°æ®å­—å…¸ä¸­
            data_test[uid][ind]['wek_n'] = torch.LongTensor(
                wek_n).unsqueeze(1)
            user_lastid[uid].append(loc[-1])  # æ›´æ–°ç”¨æˆ·çš„æœ€åä¸€ä¸ªè®¿é—®ä½ç½®

            # group_i = whole_data

            group_i = group.loc[test_inds_split[ind -
                                                len(train_inds_split)]]	 # è·å–å½“å‰æµ‹è¯•ä¼šè¯å¯¹åº”çš„æ•°æ®

            current_loc = group_i['venueid'].values  # è·å–å½“å‰ä¼šè¯çš„ä½ç½®æ•°æ®
            # å°†å½“å‰ä½ç½®æ•°æ®è½¬æ¢ä¸ºå¼ é‡ï¼Œå¹¶å¢åŠ ä¸€ä¸ªç»´åº¦ï¼Œå­˜å‚¨åœ¨æµ‹è¯•æ•°æ®å­—å…¸ä¸­
            data_test[uid][ind]['current_loc'] = torch.LongTensor(
                current_loc).unsqueeze(1)
            # ?ä¸Šé¢è¿™è¡Œä¸ºä»€ä¹ˆï¼Ÿ-ä½œä¸ºåºåˆ—è¾“å…¥åˆ°æ¨¡å‹ä¸­

            current_cat = group_i['catid'].values  # è·å–å½“å‰ä¼šè¯çš„ç±»åˆ«æ•°æ®

            current_wek_n = group_i['week_n_list'].values
            data_test[uid][ind]['current_wek_n'] = torch.LongTensor(
                current_wek_n).unsqueeze(1)  # !Test

            test_x.append(current_loc[:-1])  # å°†ä½ç½®æ•°æ®ï¼ˆé™¤æœ€åä¸€ä¸ªä»¥å¤–ï¼‰æ·»åŠ åˆ°æµ‹è¯•ä½ç½®åˆ—è¡¨
            test_x_cat.append(current_cat[:-1])  # å°†ç±»åˆ«æ•°æ®ï¼ˆé™¤æœ€åä¸€ä¸ªä»¥å¤–ï¼‰æ·»åŠ åˆ°æµ‹è¯•ç±»åˆ«åˆ—è¡¨
            test_x_week_n.append(current_wek_n[:-1])
            test_y.append(current_loc[1:])  # å°†ç›®æ ‡ä½ç½®æ•°æ®ï¼ˆå³ä¸‹ä¸€ä¸ªä½ç½®ï¼‰æ·»åŠ åˆ°æµ‹è¯•ç›®æ ‡åˆ—è¡¨
            test_hour.append(group_i['hour'].values[:-1])
            test_userid.append(uid)  # æ·»åŠ ç”¨æˆ·IDåˆ°æµ‹è¯•ç”¨æˆ·IDåˆ—è¡¨
            test_hour_pre.append(group_i['hour'].values[1:])
            test_week_pre.append(group_i['week'].values[1:])
            test_indexs.append(group_i.index.values)  # å°†å½“å‰æµ‹è¯•ä¼šè¯çš„ç´¢å¼•æ·»åŠ åˆ°æµ‹è¯•ç´¢å¼•åˆ—è¡¨

            # current_scc = group_i['sccode'].values
            # ! è¿™å„¿å°‘äº†[:-1] å¯¼è‡´åæœŸshapeç¬¬äºŒç»´å˜æˆ10
            # test_x_sccode.append(current_scc[:-1])

    with open('../data/data_train.pk', 'wb') as f:  # åºåˆ—åŒ–è®­ç»ƒæ•°æ®å¹¶å­˜å‚¨åˆ°æ–‡ä»¶
        pickle.dump(data_train, f)
    with open('../data/data_test.pk', 'wb') as f:  # åºåˆ—åŒ–æµ‹è¯•æ•°æ®å¹¶å­˜å‚¨åˆ°æ–‡ä»¶
        pickle.dump(data_test, f)
    with open('../data/train_idx.pk', 'wb') as f:  # åºåˆ—åŒ–è®­ç»ƒæ•°æ®ç´¢å¼•å¹¶å­˜å‚¨åˆ°æ–‡ä»¶
        pickle.dump(train_idx, f)
    with open('../data/test_idx.pk', 'wb') as f:   # åºåˆ—åŒ–æµ‹è¯•æ•°æ®ç´¢å¼•å¹¶å­˜å‚¨åˆ°æ–‡ä»¶
        pickle.dump(test_idx, f)

    print('user_num: ', len(data_train.keys()))  # æ‰“å°è®­ç»ƒæ•°æ®ä¸­ç”¨æˆ·çš„æ•°é‡
    # minMax = MinMaxScaler()
    # long_term_feature = minMax.fit_transform(np.array(long_term_feature))

    with open('../data/long_term.pk', 'wb') as f:  # åºåˆ—åŒ–é•¿æœŸç‰¹å¾æ•°æ®å¹¶å­˜å‚¨åˆ°æ–‡ä»¶
        pickle.dump(long_term, f)

    # with open('long_term_feature.pk','wb') as f:
    # pickle.dump(long_term_feature,f)

    # æ£€æŸ¥å‡½æ•°ï¼šç”¨äºç¡®ä¿æ‰€æœ‰è¾“å…¥åˆ—è¡¨çš„é•¿åº¦ä¸€è‡´
    def check_lengths(*arrays):
        reference_length = len(arrays[0])
        for index, array in enumerate(arrays, start=1):
            if len(array) != reference_length:
                print(
                    f"è¾“å…¥æ•°æ®é•¿åº¦ä¸ä¸€è‡´ï¼šç¬¬ {index} ä¸ªè¾“å…¥çš„é•¿åº¦ä¸º {len(array)}ï¼Œé¢„æœŸé•¿åº¦ä¸º {reference_length}")
                return False
        return True

    # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåˆ›å»ºæ•°æ®åŠ è½½å™¨
    def dataloader(X, X_cat, X_wekn, Y, hour, userid, hour_pre, week_pre):
        # æ£€æŸ¥è¾“å…¥æ•°æ®çš„ä¸€è‡´æ€§
        if not check_lengths(X, X_cat, X_wekn, Y, hour, userid, hour_pre, week_pre):
            raise ValueError("è¾“å…¥æ•°æ®é•¿åº¦ä¸ä¸€è‡´ï¼Œæ— æ³•ç»§ç»­è¿›è¡Œã€‚")
        try:
            # è½¬æ¢ä¸º NumPy æ•°ç»„
            X = np.array(X)
            X_cat = np.array(X_cat)
            X_wekn = np.array(X_wekn)
            Y = np.array(Y)
            hour = np.array(hour)
            userid = np.array(userid)
            hour_pre = np.array(hour_pre)
            week_pre = np.array(week_pre)
        except ValueError as e:
            print("æ•°æ®è½¬æ¢ä¸º NumPy æ•°ç»„æ—¶å‡ºé”™ï¼š", e)
            # æ‰“å°å…·ä½“é”™è¯¯çš„åˆ—è¡¨é•¿åº¦ä»¥è¿›è¡Œè°ƒè¯•
            print("X  :  ", [len(x) for x in X])
            print("X_cat:", [len(x) for x in X_cat])
            print("X_wekn:", [len(x) for x in X_wekn])
            print("Y:", len(Y))
            print("Y:", [len(y) for y in Y])
            print("hour:", len(hour))
            print("userid:", len(userid))
            print("hour_pre:", len(hour_pre))
            print("week_pre:", len(week_pre))
            raise

        torch_dataset = Data.TensorDataset(torch.LongTensor(X), torch.LongTensor(X_cat), torch.LongTensor(X_wekn), torch.LongTensor(
            Y), torch.LongTensor(hour), torch.LongTensor(userid), torch.LongTensor(hour_pre), torch.LongTensor(week_pre))

        # åˆ†æˆå°æ‰¹æ¬¡ï¼ˆbatchesï¼‰
        loader = Data.DataLoader(
            dataset=torch_dataset,  # è®¾ç½®æ•°æ®é›†
            batch_size=batch_size,  # è®¾ç½®æ‰¹å¤„ç†å¤§å°
            shuffle=True,  # è®¾ç½®ä¸ºéšæœºæ‰“ä¹±æ•°æ®			#TODOshuffle #BUGæ˜¯å¦éšæœºæ‰“ä¹±,æµ‹è¯•å¯¹ç»“æœçš„å½±å“
            # shuffle=True è¡¨ç¤ºåœ¨æ¯ä¸ªepochå¼€å§‹æ—¶ï¼Œæ•´ä¸ªæ•°æ®é›†çš„æ ·æœ¬é¡ºåºå°†è¢«éšæœºæ‰“ä¹±
            num_workers=0,  # è®¾ç½®å·¥ä½œçº¿ç¨‹æ•°
        )
        return loader  # è¿”å›æ•°æ®åŠ è½½å™¨

    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
    loader_train = dataloader(train_x, train_x_cat, train_x_week_n,
                              train_y, train_hour, train_userid, train_hour_pre, train_week_pre)

    loader_test = dataloader(test_x, test_x_cat, test_x_week_n,
                             test_y, test_hour, test_userid, test_hour_pre, test_week_pre)

    pre_data = {}  # åˆå§‹åŒ–ä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨é¢„å¤„ç†æ•°æ®
    pre_data['size'] = [vocab_size_poi, vocab_size_cat, vocab_size_week_n,
                        vocab_size_user, len(train_x), len(test_x)]  # å­˜å‚¨ç›¸å…³å°ºå¯¸ä¿¡æ¯
    pre_data['loader_train'] = loader_train  # å­˜å‚¨è®­ç»ƒæ•°æ®åŠ è½½å™¨
    pre_data['loader_test'] = loader_test  # å­˜å‚¨æµ‹è¯•æ•°æ®åŠ è½½å™¨

    with open('../data/pre_data.txt', 'wb') as f:  # åºåˆ—åŒ–é¢„å¤„ç†æ•°æ®å¹¶å­˜å‚¨åˆ°æ–‡ä»¶
        pickle.dump(pre_data, f)
    return pre_data  # è¿”å›é¢„å¤„ç†æ•°æ®


def geodistance(lng1, lat1, lng2, lat2):
    lng1, lat1, lng2, lat2 = map(
        radians, [float(lng1), float(lat1), float(lng2), float(lat2)])
    dlon = lng2-lng1
    dlat = lat2-lat1
    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
    distance = 2*asin(sqrt(a))*6371*1000
    distance = round(distance/1000, 3)
    return distance


def caculate_poi_distance(poi_coors, start_x_index=0, start_y_index=0):
    print("distance matrix")
    print(len(poi_coors))

    epsilon = 1e6  # ä¸€ä¸ªå¾ˆå°çš„æ­£æ•°ï¼Œé¿å…é™¤é›¶é”™è¯¯
    sim_matrix = np.zeros((len(poi_coors) + 1, len(poi_coors) + 1))
    try:
        #with open('../data/datadistance_1type.txt', 'w') as f:
        max_dis = 0
        for i in tqdm(range(len(poi_coors))):
            for j in tqdm(range(i, len(poi_coors)), leave=False):
                poi_current = i
                poi_target = j
                poi_current_coor = poi_coors[poi_current]
                poi_target_coor = poi_coors[poi_target]
                if poi_current == poi_target:
                    distance_between = epsilon  # è®¾ç½®å¯¹è§’çº¿ä¸Šçš„è·ç¦»ä¸ºä¸€ä¸ªå¾ˆå¤§çš„æ­£æ•°
                else:
                    distance_between = geodistance(
                        poi_current_coor[0], poi_current_coor[1], poi_target_coor[0], poi_target_coor[1])
                    if distance_between > max_dis:  max_dis = distance_between
                sim_matrix[poi_current][poi_target] = distance_between
                sim_matrix[poi_target][poi_current] = distance_between
                #f.write(f"{poi_current},{poi_target},{distance_between}\n")
        print("max_distance_between_poi: ",max_dis)
    except Exception as e:
        print(f"Error occurred: {e}")
    # pickle.dump(sim_matrix, open('../data/datadistance_1type.pkl', 'wb'))
    print("already save the distance matrix")

    return sim_matrix


def excute_caculate_poi_distance(df):
    print("function: excute_caculate_poi_distance is running!")
    # df = pd.read_table('../data/user_1type_all.txt', header=None)
    df = df.drop_duplicates(subset=[1], keep='first')
    poi_coordinate = [(float(row[3]), float(row[4]))
                      for _, row in df.iterrows()]
    # unique_poi_coordinate = list(set(poi_coordinate))
    coorlen = len(poi_coordinate)
    print("coorlen: ", coorlen)
    start_x_index = 0
    start_y_index = 0
    sim_matrix = caculate_poi_distance(
        poi_coordinate, start_x_index, start_y_index)
    print("poi_distance_matrix.shape: ",sim_matrix.shape)
    pickle.dump(sim_matrix, open('../data/datadistance_1type_2.pkl', 'wb'))

    '''
    # ä¸­æ–­æ¢å¤
    # line_count = sum(1 for line in open('../data/datadistance_1type.txt', 'r'))
    # s_x = line_count % coorlen
    # s_y = line_count - s_x*coorlen - 1
    # sim_matrix = caculate_poi_distance(
    #     unique_poi_coordinate, s_x+1, s_y)
    # pickle.dump(sim_matrix, open('../data/datadistance_1type.pkl', 'wb'))

    # è¯»å–txtæ–‡ä»¶ä¸­çš„è®¡ç®—ç»“æœä¿å­˜åˆ°pklæ–‡ä»¶ä¸­
    '''


def caculate_venue_frequency(df_vid):
    # Step 1: è¯»å–å’Œé¢„å¤„ç†æ•°æ®
    print(df_vid.head())
    df_vid_unique = df_vid.drop_duplicates(subset=[1], keep='first')
    # TODO .loc[:,]åˆ°åº•ä»€ä¹ˆç”¨ï¼Œcatid_time_matrixé‚£è¾¹ä¸è¡ŒğŸ™…å•Š
    df_vid.loc[:, 5] = pd.to_datetime(df_vid.loc[:, 5])
    print("df_vid len: ", len(df_vid))
    # 2012å’Œ2013éƒ½æ˜¯éƒ½52å‘¨ 1-52 2 0-51
    df_vid.loc[:, 7] = df_vid.loc[:, 5].dt.isocalendar().week

    # Step 2: è®¡ç®—æ¯ä¸ª VenueID åœ¨æ¯å‘¨çš„è®¿é—®é¢‘æ¬¡
    venue_freq_matrix = np.zeros((53, len(df_vid_unique) + 1))
    venue_dict = {}
    total_rows = len(df_vid)
    with tqdm(total=total_rows, desc="Processing rows") as pbar:
        i = 0
        for _, row in df_vid.iterrows():
            week = row[7]
            if row[1] not in venue_dict:
                venue_dict[row[1]] = i
                venue_freq_matrix[week][i] += 1
                i += 1
            else:
                venue_freq_matrix[week][venue_dict[row[1]]] += 1
            pbar.update(1)
    print("venue_dict len: ", len(venue_dict))
    print("venue_freq_matrix shape: ", venue_freq_matrix.shape)

    return venue_freq_matrix


def excute_caculate_venue_frequency(df):
    # df_vid = pd.read_table('../data/user_1type_all.txt', header=None)
    df_vid = df
    venue_freq_matrix = caculate_venue_frequency(df_vid)
    print("venue_freq_matrix.shape: ",venue_freq_matrix.shape)
    pickle.dump(venue_freq_matrix, open('../data/venue_freq_matrix.pkl', 'wb'))
    print("venue_freq_matrix.pkl is Done")
    # å°† np çŸ©é˜µ venue_freq_matrix ä¿å­˜åˆ° txt æ–‡ä»¶
    np.savetxt('../data/venue_freq_matrix.txt',
               venue_freq_matrix, fmt='%d', delimiter='\t')
    print("venue_freq_matrix.txt is Done")


def caculate_time_cid(df):
    df_catid_unique = df.drop_duplicates(subset=[2], keep='first')
    print("len df_catid_unique: ", len(df_catid_unique))
    sim_matrix = np.zeros((48, len(df_catid_unique)+1))  # TODO 48,242

    df[5] = pd.to_datetime(df[5], errors='coerce')
    # # æ£€æŸ¥è½¬æ¢æ˜¯å¦æˆåŠŸ
    # if df.loc[:, 5].isnull().any():
    #     raise ValueError("Error in converting column 5 to datetime format")
    df[7] = df[5].dt.hour
    df[8] = df[5].dt.weekday

    catid_dict = {}
    total_rows = len(df)
    with tqdm(total=total_rows, desc="Processing rows") as pbar:
        i = 0
        for _, row in df.iterrows():
            hour = row[7]  # 0-47
            if row[8] == 0 or row[8] == 6:
                hour += 24
            if row[2] not in catid_dict:
                catid_dict[row[2]] = i
                sim_matrix[hour][i] += 1
                i += 1
            else:
                sim_matrix[hour][catid_dict[row[2]]] += 1
            pbar.update(1)
    print(sim_matrix.shape)
    return sim_matrix


def excute_caculate_time_cid(df):
    # df = pd.read_table('../data/user_1type_all.txt', header=None)
    catid_time_matrix = caculate_time_cid(df)
    pickle.dump(catid_time_matrix, open('../data/catid_time_matrix.pkl', 'wb'))
    print("catid_time_matrix.pkl is Done")
    # å°† np çŸ©é˜µ catid_time_matrix ä¿å­˜åˆ° txt æ–‡ä»¶
    np.savetxt('../data/catid_time_matrix.txt',
               catid_time_matrix, fmt='%d', delimiter='\t')
    print("catid_time_matrix.txt is Done")
